<!--
Google IO 2012 HTML5 Slide Template

Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>

URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title> Protein Folding is Easy</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
  <!--This one seems to work all the time, but really small on ipad-->
  <!--<meta name="viewport" content="initial-scale=0.4">-->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/custom.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides" src="js/require-1.0.8.min.js"></script>

  <!-- MathJax support  -->
  <script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

<!-- <slide class="logoslide nobackground">
  <article class="flexbox vcenter">
    <span><img src="images/google_developers_logo.png"></span>
  </article>
</slide>
 -->
<slide class="title-slide segue nobackground">
  <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
  <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
  <hgroup class="auto-fadein">

    <h1> Protein Folding is Easy</h1>
    <h2> Towards MSMs for Conformational Change</h2>
    <p> Robert T. McGibbon<br/> April 22, 2013<br/></p>
  </hgroup>
</slide>


<slide  >
  
    <hgroup>
      <h2>A Presentation in Three Acts</h2>
      <h3></h3>
    </hgroup>
    <article ><div style="padding:50px"></div>

<ul>
<li>Resolving conformation slow conformation changes within a folding data set.</li>
<li>MSMAccelerator: Adaptive sampling</li>
<li>OpenMM Script Builder</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MSM Parameter Selection</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>MSM construction is a mix of supervised and unsupervised learning problems.</li>
<li>Unsupervised learning is the problem of finding "hidden" structure in unlabeled data, where there's no <em>right</em> answer.</li>
<li>How many conformational states does a protein adopt? The answer is in the eye of the beholder.</li>
<li>Parameterizing a transition matrix is supervised.</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MSM State Decomposition</h2>
      <h3></h3>
    </hgroup>
    <article ><p>The MSM state decomposition, <em>clustering</em>, is characterized by a bias-variance trade off.</p>
<ul>
<li><strong>Bias:</strong> As you lower the number of states, you introduce systematic error in modeling the dynamics.</li>
<li>Hamiltonian dynamics are completely Markovian in $\mathbb{R}^{6N}$</li>
<li><strong>Variance:</strong> As you raise the number of states, you're increasing subject to statistical noise in the transition matrix estimation.</li>
<li>How do we balance this trade off?</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Choosing the states' shape</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height=150 src=figures/gpcr_activation.png /></p>
<ul>
<li>Conformational change is characterized by slow <em>conformationally subtle</em>
  transitions.</li>
<li>To resolve these transitions in our models, our states need to be "smaller".</li>
<li>Increasing the number of states is not the <em>only</em> way to lower the bias --
  we can also pick the <strong>shape</strong> of our states more intelligently.</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Protein motions aren't isotropic</h2>
      <h3>Our MSM states shouldn't be either</h3>
    </hgroup>
    <article ><p><img height=150 src=figures/gpcr_activation.png /></p>
<ul>
<li>Different structural degrees of freedom should be weighted according to their
  discriminatory power (equilibration rate).</li>
<li>Learn a distance metric for clustering which maximally separates kinetically
close and kinetically distant conformations.</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Large-Margin Learning</h2>
      <h3></h3>
    </hgroup>
    <article ><table class="flexbox vcenter"><tr>
  <td><img height=150 src="figures/classifiers.png"></td>
  <td><img height=150 src="figures/classifiers2.png"></td>
</tr></table>

<ul>
<li>A common goal in supervised learning is to construct binary classifiers,
  e.g. actives vs. inactives, cats vs. others.</li>
<li>The "margin" is the distance of the object's score from the the decision threshold.</li>
<li>Large margin approaches attempt to find a classifier via optimization methods
  that maximize the margins.</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Large-Margin Distance Metric (KDML)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>A set of $N$ triplets of structres, $(a, b, c)$, where $a$ and
  $b$ appear close together in a single traj., while $a$ and $c$ don't.</li>
<li>Find a squared Mahalanobis metrics, and maximize the margin
  between the close and far pairs.</li>
</ul>
<p>$$ d^{\mathbf{X}}(\vec{a}, \vec{b}) = (\vec{a} - \vec{b})^{T} \mathbf{X} (\vec{a} - \vec{b}) $$</p>
<p>$$  \max_{\mathbf{X},\rho} \left[ \alpha \rho - \frac{1}{N} \sum_i^N \lambda \left(d^\mathbf{X}(\vec{a}_i,\vec{c}_i) - d^\mathbf{X}(\vec{a}_i, \vec{b}_i) - \rho \right) \right]
$$</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Optimization and Constraints</h2>
      <h3></h3>
    </hgroup>
    <article ><p>$$ \max_{\mathbf{X},\rho} \left[ \alpha \rho - \frac{1}{N} \sum_i^N \lambda \left(d^\mathbf{X}(\vec{a}_i,\vec{c}_i) - d^\mathbf{X}(\vec{a}_i, \vec{b}_i) - \rho \right) \right] $$</p>
<ul>
<li>The matrix $\mathbf{X}$ is constrained to be positive semidefinite.</li>
<li>Efficient optimization by gradient descent with rank-1 updates -- maintains p.s.d.</li>
<li>Shen, C.; Kim, J.; Wang, L. Scalable large-margin Mahalanobis distance metric learning.
  <em>IEEE</em> <em>Trans.</em> <em>Neural</em> <em>Networks</em> <strong>2010</strong>, 21, 1524–1530</li>
</ul></article>
 
</slide>

<slide class="img-top-center" >
  
    <hgroup>
      <h2>Model System</h2>
      <h3></h3>
    </hgroup>
    <article ><p><img height=250 src="figures/toy_microstates.png"></p>
<ul>
<li>2D Brownian dynamics, where vertical diffusion constant is 10x greater than
the horizontal diffusion constant.</li>
</ul>
<p><span style=font-size:30px>
  $$\mathbf{X} = \begin{pmatrix} 0.9915 &amp; 0.0 \cr 0.0 &amp; 0.0085 \end{pmatrix}$$
</span></p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Model System</h2>
      <h3></h3>
    </hgroup>
    <article ><div class="flexbox vcenter">
<img src="figures/timescales.png">
</div>

<p>KDML distance metric gives more converged timescales with fewer states.</p></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Fip35 WW Domain (Shaw)</h2>
      <h3>Lets look at some <em>real</em> data</h3>
    </hgroup>
    <article ><div class="vcenter flexbox">
<div style="width:290px"><img height=150 src="figures/tocfigure.png"></div>
</div>

<ul>
<li>Sampled k=20,000 triplets at $t_{close}$ = 2 ns, $t_{far}$ = 20 ns</li>
<li>Structures projected onto the sine and cosine components of the backbone
  dihedrals.</li>
</ul>
<footer class="source">Shaw et. al; Atomic-level characterization of the structural dynamics of proteins. <em>Science</em> <strong>2010</strong>, 330, 341–346</footer></article>
 
</slide>

<slide class="image" >
  
    <hgroup>
      <h2>Fip35 WW Domain (Shaw)</h2>
      <h3>n states: 5000, lagtime: 75 ns</h3>
    </hgroup>
    <article ><div class="vcenter flexbox">
<div> <img height=450  src="figures/bars.png"> </div>
</div></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Unweighted metrics miss slow near-native dynamics</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>The folding timescale is remarkably robust to changes in the distance metric.</li>
<li>New timescales are observed in the 100 ns - 1 μs regime, corresponding 
  to near-native hydrogen bond reorganizations in the turns.</li>
</ul>
<div class="vcenter flexbox">
<div><img height=250 src="figures/state12.png"></div>
</div></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Unweighted metrics miss slow near-native dynamics</h2>
      <h3></h3>
    </hgroup>
    <article ><div class="vcenter flexbox">
<div><img height=475 src="figures/hbonds_01.png"></div>
</div></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MSM State Decomposition Outlook</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>This <strong>is</strong> the right direction, but not necessarily the right algorithm.</li>
<li>Numerical optimization of the metric challenging &amp; can be poorly conditioned.</li>
<li>Low rank metric = dimensionality reduction.<ul>
<li>Enables computational access to more sophisticated clustering.</li>
</ul>
</li>
<li><strong>Sparse</strong> metric = feature selection.<ul>
<li>Promising for biophysical insight.</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>MSMAccelerator</h2>
      <h3>Distributed Adaptive Sampling</h3>
    </hgroup>
  
</slide>

<slide  >
  
    <hgroup>
      <h2>MSMAccelerator Architecture</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>Distributed messaging passing design (<a href="http://learning-0mq-with-pyzmq.readthedocs.org/en/latest/">ZeroMQ</a>).</li>
<li>Two types of clients can connect to the adaptive server:<ul>
<li><strong>Simulator</strong>: Receives initial conditions, propagates dynamics.</li>
<li><strong>Modeler</strong>: Receives trajectory data, builds an MSM.</li>
</ul>
</li>
<li>Adaptive server maintains weights for a multinomial distribution, from the
  most recent MSM.</li>
<li>Setup system with serialized OpenMM XML files.</li>
<li>Code on <a href="http://github.com/rmcgibbo/msmaccelerator2">github</a>.</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Adaptive Sampling Algorithms</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>How do we switch between sampling strategies?<ul>
<li>Voelz surpisal strategy / mutual information between state decompositions.</li>
<li>Smooth interpolation, simulated annealing</li>
</ul>
</li>
<li>The explore/exploit tradeoff has strong overlap with the multi-armed bandit,
  and probabalistic multi-robot mapping problems.</li>
<li>MSMAccelerator provides the rapid prototyping capability.</li>
</ul>
<footer class="source">
  S. Thrun, Exploration in Active Learning <strong>1998</strong>
</footer></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>MSMAccelerator</h2>
      <h3></h3>
    </hgroup>
    <article ><video class="center" height=350 controls>
  <source src="figures/movie.ogv" type="video/ogg">
</video>

<p><div style="padding:0px"></div></p>
<p>Ala5 (amber99sbiln, implicit) OpenMM 5.1 / MSMBuilder2.6. 1000 rounds
(1μs aggregate) of adaptive sampling (even).</p></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>OpenMM Script Builder</h2>
      <h3>Effortless OpenMM simulation <a href="http://openmm.heroku.com">setup</a></h3>
    </hgroup>
  
</slide>


<slide class="thank-you-slide segue nobackground">
  <!-- <aside class="gdbar right"><img src="images/google_developers_icon_128.png"></aside> -->
  <article class="flexbox vleft auto-fadein">
    <h2> Thanks everyone!</h2>
    <p> And especially Vijay, Christian and Kyle.</p>
  </article>
  <!-- <p class="auto-fadein" data-config-contact> -->
  <p data-config-contact class="auto-fadein"> <span>www</span> <a href="http://www.stanford.edu/~rmcgibbo">stanford.edu/~rmcgibbo</a><br/> <span>github</span> <a href="http://github.com/rmcgibbo">github.com/rmcgibbo</a></p>
  <!-- <p data-config-contact> -->
    <!-- populated from slide_config.json -->
  </p>
</slide>

<!-- <slide class="logoslide dark nobackground">
  <article class="flexbox vcenter">
    <span><img src="images/google_developers_logo_white.png"></span>
  </article>
</slide> -->

<slide class="backdrop"></slide>

</slides>

<script>
var _gaq = _gaq || [];
_gaq.push(['_setAccount', 'UA-XXXXXXXX-1']);
_gaq.push(['_trackPageview']);

(function() {
  var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
  ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
})();
</script>

<!--[if IE]>
  <script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
  <script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>